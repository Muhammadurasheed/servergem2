"""
ServerGem Orchestrator Agent
FAANG-Level Production Implementation
- Gemini ADK with function calling
- Production monitoring & observability
- Security best practices
- Cost optimization
- Advanced error handling
"""

import asyncio
import time
from typing import Dict, List, Optional
import google.generativeai as genai
from datetime import datetime
import json
import uuid

class OrchestratorAgent:
    """
    Production-grade orchestrator using Gemini ADK with function calling.
    Routes to real services: GitHub, Google Cloud, Docker, Analysis.
    """
    
    def __init__(self, gemini_api_key: str, github_token: str = None, gcloud_project: str = None):
        genai.configure(api_key=gemini_api_key)
        
        # ServerGem-specific system instruction
        system_instruction = """
You are ServerGem AI Assistant - a production-grade AI that deploys applications to Google Cloud Run using ServerGem's managed infrastructure.

CRITICAL ARCHITECTURE PRINCIPLES:
- Users do NOT need Google Cloud accounts or gcloud authentication
- Users do NOT need to provide project IDs or service account keys
- ServerGem handles ALL Google Cloud interactions using its own managed infrastructure
- You deploy everything to ServerGem's platform and provide custom .servergem.app URLs

NEVER ask users for:
âŒ Google Cloud project IDs
âŒ Service account keys
âŒ gcloud CLI authentication commands
âŒ IAM permissions or roles

ALWAYS ask for:
âœ… GitHub repository URL
âœ… Service name (suggest based on repo name)
âœ… Environment variables (if needed for the app)

DEPLOYMENT FLOW:
1. User provides GitHub repo URL
2. You clone and analyze the repository
3. You generate optimal Dockerfile
4. You deploy to ServerGem's Cloud Run infrastructure
5. You provide custom URL: https://{service-name}.servergem.app

Be concise, helpful, and NEVER mention gcloud setup or GCP authentication.
        """.strip()
        
        # Initialize Gemini with function declarations and system instruction
        self.model = genai.GenerativeModel(
            'gemini-2.0-flash-exp',
            tools=[self._get_function_declarations()],
            system_instruction=system_instruction
        )
        
        self.conversation_history: List[Dict] = []
        self.project_context: Dict = {}
        self.chat_session = None
        
        # Initialize real services
        from services import GitHubService, GCloudService, DockerService, AnalysisService
from services.monitoring import monitoring
        from services.security import security
        from services.optimization import optimization
        from services.deployment_progress import create_progress_tracker
        
        self.github_service = GitHubService(github_token)
        # Use ServerGem's GCP project (not user's)
        self.gcloud_service = GCloudService(gcloud_project or 'servergem-platform') if gcloud_project else None
        self.docker_service = DockerService()
        self.analysis_service = AnalysisService(gemini_api_key)
        
        # Production services
        self.monitoring = monitoring
        self.security = security
        self.optimization = optimization
    
    def _get_function_declarations(self) -> List[Dict]:
        """
        Define real functions available for Gemini ADK to call
        Uses proper Google AI SDK format
        """
        return [
            {
                'name': 'clone_and_analyze_repo',
                'description': 'Clone a GitHub repository and perform comprehensive analysis to detect framework, dependencies, and deployment requirements. Use this when user provides a GitHub repo URL.',
                'parameters': {
                    'type': 'object',
                    'properties': {
                        'repo_url': {
                            'type': 'string',
                            'description': 'GitHub repository URL (https://github.com/user/repo or git@github.com:user/repo.git)'
                        },
                        'branch': {
                            'type': 'string',
                            'description': 'Branch name to clone and analyze (default: main)'
                        }
                    },
                    'required': ['repo_url']
                }
            },
            {
                'name': 'deploy_to_cloudrun',
                'description': 'Deploy an analyzed project to Google Cloud Run. Generates Dockerfile, builds image via Cloud Build, and deploys the service. Use this after analyzing a repository.',
                'parameters': {
                    'type': 'object',
                    'properties': {
                        'project_path': {
                            'type': 'string',
                            'description': 'Local path to the cloned project (from project_context)'
                        },
                        'service_name': {
                            'type': 'string',
                            'description': 'Name for the Cloud Run service (lowercase, hyphens allowed)'
                        },
                        'env_vars': {
                            'type': 'object',
                            'description': 'Environment variables as key-value pairs (optional)'
                        }
                    },
                    'required': ['project_path', 'service_name']
                }
            },
            {
                'name': 'list_user_repositories',
                'description': 'List GitHub repositories for the authenticated user. Use this when user asks to see their repos or wants to select a project to deploy.',
                'parameters': {
                    'type': 'object',
                    'properties': {},
                    'required': []
                }
            },
            {
                'name': 'get_deployment_logs',
                'description': 'Fetch recent logs from a deployed Cloud Run service. Use this for debugging deployment issues or when user asks to see logs.',
                'parameters': {
                    'type': 'object',
                    'properties': {
                        'service_name': {
                            'type': 'string',
                            'description': 'Cloud Run service name'
                        },
                        'limit': {
                            'type': 'integer',
                            'description': 'Number of log entries to fetch (default: 50)'
                        }
                    },
                    'required': ['service_name']
                }
            }
        ]
    
    async def process_message(self, user_message: str, session_id: str, progress_callback=None) -> Dict:
        """
        Main entry point: processes user message with Gemini ADK function calling
        
        Args:
            user_message: User's chat message
            session_id: Session identifier
            progress_callback: Optional async callback for real-time updates
        """
        
        # Initialize chat session if needed
        if not self.chat_session:
            self.chat_session = self.model.start_chat(history=[])
        
        # Add project context to enhance Gemini's understanding
        context_prefix = self._build_context_prefix()
        enhanced_message = f"{context_prefix}\n\nUser: {user_message}" if context_prefix else user_message
        
        try:
            # Send to Gemini with function calling enabled
            response = await asyncio.to_thread(
                self.chat_session.send_message,
                enhanced_message
            )
            
            # Check if Gemini wants to call a function
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        if hasattr(part, 'function_call') and part.function_call:
                            # Route to real service handler
                            return await self._handle_function_call(
                                part.function_call,
                                progress_callback=progress_callback
                            )
            
            # Regular text response (no function call needed)
            response_text = ''
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                parts = response.candidates[0].content.parts
                if parts:
                    response_text = ''.join([part.text for part in parts if hasattr(part, 'text')])
            
            return {
                'type': 'message',
                'content': response_text if response_text else 'I received your message but couldn\'t generate a response. Please try again.',
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"[Orchestrator] Error: {str(e)}")
            return {
                'type': 'error',
                'content': f'âŒ Error processing message: {str(e)}',
                'timestamp': datetime.now().isoformat()
            }
    
    async def _handle_function_call(self, function_call, progress_callback=None) -> Dict:
        """
        Route Gemini function calls to real service implementations
        
        Args:
            function_call: Gemini function call object
            progress_callback: Optional async callback for WebSocket updates
        """
        
        function_name = function_call.name
        args = dict(function_call.args)
        
        print(f"[Orchestrator] Function call: {function_name} with args: {args}")
        
        # Route to real service handlers
        handlers = {
            'clone_and_analyze_repo': self._handle_clone_and_analyze,
            'deploy_to_cloudrun': self._handle_deploy_to_cloudrun,
            'list_user_repositories': self._handle_list_repos,
            'get_deployment_logs': self._handle_get_logs
        }
        
        handler = handlers.get(function_name)
        
        if handler:
            return await handler(progress_callback=progress_callback, **args)
        else:
            return {
                'type': 'error',
                'content': f'âŒ Unknown function: {function_name}',
                'timestamp': datetime.now().isoformat()
            }
    
    # ========================================================================
    # REAL SERVICE HANDLERS - Production Implementation
    # ========================================================================
    
    async def _handle_clone_and_analyze(self, repo_url: str, branch: str = 'main', progress_callback=None) -> Dict:
        """
        Clone GitHub repo and analyze it - REAL IMPLEMENTATION
        Uses: GitHubService, AnalysisService, DockerService
        """
        
        try:
            # Create progress tracker for structured updates
            tracker = create_progress_tracker(
                deployment_id=f"analysis-{datetime.now().timestamp()}",
                service_name=repo_url.split('/')[-1].replace('.git', ''),
                progress_callback=progress_callback
            )
            
            # Step 1: Clone repository using GitHubService
            await tracker.start_repo_clone(repo_url)
            
            clone_result = self.github_service.clone_repository(repo_url, branch)
            
            if not clone_result.get('success'):
                return {
                    'type': 'error',
                    'content': f"âŒ **Failed to clone repository**\n\n{clone_result.get('error')}\n\nPlease check:\nâ€¢ Repository URL is correct\nâ€¢ You have access to the repository\nâ€¢ GitHub token has proper permissions",
                    'timestamp': datetime.now().isoformat()
                }
            
            project_path = clone_result['local_path']
            self.project_context['project_path'] = project_path
            self.project_context['repo_url'] = repo_url
            self.project_context['branch'] = branch
            
            # Emit clone completion with details
            await tracker.complete_repo_clone(
                project_path,
                clone_result.get('files_count', 0),
                clone_result.get('size_mb', 0.0)
            )
            
            # Step 2: Analyze project using AnalysisService
            await tracker.start_code_analysis(project_path)
            
            analysis_result = await self.analysis_service.analyze_and_generate(project_path)
            
            if not analysis_result.get('success'):
                await tracker.emit_error('code_analysis', analysis_result.get('error', 'Unknown error'))
                return {
                    'type': 'error',
                    'content': f"âŒ **Analysis failed**\n\n{analysis_result.get('error')}",
                    'timestamp': datetime.now().isoformat()
                }
            
            # Emit framework and dependency detection
            analysis_data = analysis_result['analysis']
            await tracker.emit_framework_detection(
                analysis_data['framework'],
                analysis_data['language'],
                analysis_data.get('runtime', 'latest')
            )
            await tracker.emit_dependency_analysis(
                analysis_data['dependencies_count'],
                analysis_data.get('database')
            )
            await tracker.complete_code_analysis()
            
            # Step 3: Generate and save Dockerfile
            await tracker.start_dockerfile_generation(analysis_data['framework'])
            
            await tracker.emit_dockerfile_optimization(
                analysis_result['dockerfile']['optimizations']
            )
            
            dockerfile_save = self.docker_service.save_dockerfile(
                analysis_result['dockerfile']['content'],
                project_path
            )
            
            await tracker.complete_dockerfile_generation(
                dockerfile_save.get('path', f'{project_path}/Dockerfile')
            )
            
            # Step 4: Create .dockerignore
            self.docker_service.create_dockerignore(
                project_path,
                analysis_result['analysis']['language']
            )
            
            # Store analysis in context for future operations
            self.project_context['analysis'] = analysis_result['analysis']
            self.project_context['framework'] = analysis_result['analysis']['framework']
            self.project_context['language'] = analysis_result['analysis']['language']
            
            # Format beautiful response
            analysis_data = analysis_result['analysis']
            content = f"""
ðŸ” **Analysis Complete: {repo_url.split('/')[-1]}**

**Framework:** {analysis_data['framework']} ({analysis_data['language']})
**Entry Point:** `{analysis_data['entry_point']}`
**Dependencies:** {analysis_data['dependencies_count']} packages
**Port:** {analysis_data['port']}
{f"**Database:** {analysis_data['database']}" if analysis_data.get('database') else ''}
{f"**Environment Variables:** {len(analysis_data['env_vars'])} detected" if analysis_data.get('env_vars') else ''}

âœ… **Dockerfile Generated** ({dockerfile_save.get('path', 'Dockerfile')})
{chr(10).join(['â€¢ ' + opt for opt in analysis_result['dockerfile']['optimizations'][:4]])}

ðŸ“‹ **Recommendations:**
{chr(10).join(['â€¢ ' + rec for rec in analysis_result['recommendations'][:3]])}

{f"âš ï¸ **Warnings:**{chr(10)}{chr(10).join(['â€¢ ' + w for w in analysis_result['warnings']][:2])}" if analysis_result.get('warnings') else ''}

Ready to deploy to Google Cloud Run! Would you like me to proceed?
            """.strip()
            
            return {
                'type': 'analysis',
                'content': content,
                'data': analysis_result,
                'actions': [
                    {
                        'id': 'deploy',
                        'label': 'ðŸš€ Deploy to Cloud Run',
                        'type': 'button',
                        'action': 'deploy'
                    },
                    {
                        'id': 'view_dockerfile',
                        'label': 'ðŸ“„ View Dockerfile',
                        'type': 'button',
                        'action': 'view_dockerfile'
                    },
                    {
                        'id': 'configure_env',
                        'label': 'âš™ï¸ Configure Env Vars',
                        'type': 'button',
                        'action': 'configure_env'
                    }
                ],
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            error_msg = str(e)
            print(f"[Orchestrator] Clone and analyze error: {error_msg}")
            import traceback
            traceback.print_exc()
            
            # Send error via progress callback if available
            if progress_callback:
                try:
                    from services.deployment_progress import create_progress_tracker
                    tracker = create_progress_tracker(
                        deployment_id=f"error-{datetime.now().timestamp()}",
                        service_name="analysis",
                        progress_callback=progress_callback
                    )
                    await tracker.emit_error("Repository Analysis", error_msg)
                except Exception as callback_error:
                    print(f"[Orchestrator] Could not send error via callback: {callback_error}")
                    pass
            
            return {
                'type': 'error',
                'content': f'âŒ **Analysis failed**\n\n```\n{error_msg}\n```\n\nPlease try again or check the logs.',
                'timestamp': datetime.now().isoformat()
            }
    
    async def _handle_deploy_to_cloudrun(
        self,
        project_path: str,
        service_name: str,
        env_vars: Dict = None,
        progress_callback=None
    ) -> Dict:
        """
        Deploy to Cloud Run - PRODUCTION IMPLEMENTATION
        
        Features:
        - Security validation and sanitization
        - Resource optimization based on framework
        - Cost estimation
        - Monitoring and metrics
        - Structured logging
        """
        
        if not self.gcloud_service:
            return {
                'type': 'error',
                'content': 'âŒ **ServerGem Cloud not configured**\n\nPlease contact support. This is a platform configuration issue.',
                'timestamp': datetime.now().isoformat()
            }
        
        # Generate deployment ID for tracking
        deployment_id = f"deploy-{uuid.uuid4().hex[:8]}"
        start_time = time.time()
        
        try:
            # Create progress tracker for real-time updates
            tracker = create_progress_tracker(
                deployment_id,
                service_name,
                progress_callback
            )
            
            # Start monitoring
            metrics = self.monitoring.start_deployment(deployment_id, service_name)
            
            # Security: Validate and sanitize service name
            name_validation = self.security.validate_service_name(service_name)
            if not name_validation['valid']:
                self.monitoring.complete_deployment(deployment_id, "failed")
                return {
                    'type': 'error',
                    'content': f"âŒ **Invalid service name**\n\n{name_validation['error']}\n\nRequirements:\nâ€¢ Lowercase letters, numbers, hyphens only\nâ€¢ Must start with letter\nâ€¢ Max 63 characters",
                    'timestamp': datetime.now().isoformat()
                }
            
            service_name = name_validation['sanitized_name']
            
            # Security: Validate environment variables
            if env_vars:
                env_validation = self.security.validate_env_vars(env_vars)
                if env_validation['issues']:
                    self.monitoring.record_error(
                        deployment_id,
                        f"Environment variable issues: {', '.join(env_validation['issues'])}"
                    )
                env_vars = env_validation['sanitized']
            
            # Optimization: Get optimal resource config
            framework = self.project_context.get('framework', 'unknown')
            optimal_config = self.optimization.get_optimal_config(framework, 'medium')
            
            self.monitoring.record_stage(deployment_id, 'validation', 'success', 0.5)
            
            # SERVERGEM ARCHITECTURE: No user GCP auth needed - using ServerGem's infrastructure
            # Step 1: Validate Dockerfile exists
            dockerfile_check = self.docker_service.validate_dockerfile(project_path)
            if not dockerfile_check.get('valid'):
                self.monitoring.complete_deployment(deployment_id, 'failed')
                return {
                    'type': 'error',
                    'content': f"âŒ **Invalid Dockerfile**\n\n{dockerfile_check.get('error')}",
                    'timestamp': datetime.now().isoformat()
                }
            
            # Security: Scan Dockerfile
            await tracker.start_security_scan()
            
            security_scan = self.security.scan_dockerfile_security(
                open(f"{project_path}/Dockerfile").read()
            )
            
            # Emit security check results
            await tracker.emit_security_check("Base image validation", security_scan['secure'])
            await tracker.emit_security_check("Privilege escalation check", not any('privilege' in issue.lower() for issue in security_scan['issues']))
            await tracker.emit_security_check("Secret exposure check", not any('secret' in issue.lower() for issue in security_scan['issues']))
            
            await tracker.complete_security_scan(len(security_scan['issues']))
            
            if not security_scan['secure']:
                for issue in security_scan['issues'][:3]:
                    self.monitoring.record_error(deployment_id, f"Security: {issue}")
                    await tracker.emit_warning(f"Security issue: {issue}")
            
            # Step 3: Build Docker image with Cloud Build
            image_tag = f"gcr.io/servergem-platform/{service_name}:latest"
            await tracker.start_container_build(image_tag)
            
            build_start = time.time()
            
            async def build_progress(data):
                """Forward build progress to tracker"""
                if data.get('step'):
                    await tracker.emit_build_step(
                        data['step'],
                        data.get('total_steps', 10),
                        data.get('description', 'Building...')
                    )
                elif data.get('progress'):
                    await tracker.emit_build_progress(data['progress'])
            
            build_result = await self.gcloud_service.build_image(
                project_path,
                service_name,
                progress_callback=build_progress
            )
            
            build_duration = time.time() - build_start
            self.monitoring.record_stage(deployment_id, 'build', 'success', build_duration)
            
            if not build_result.get('success'):
                await tracker.emit_error('container_build', build_result.get('error', 'Build failed'))
                self.monitoring.complete_deployment(deployment_id, 'failed')
                return {
                    'type': 'error',
                    'content': f"âŒ **Build failed**\n\n{build_result.get('error')}\n\nCheck:\nâ€¢ Dockerfile syntax\nâ€¢ Cloud Build API is enabled\nâ€¢ Billing is enabled",
                    'timestamp': datetime.now().isoformat()
                }
            
            # Emit build completion
            await tracker.complete_container_build(
                build_result.get('image_digest', 'sha256:' + deployment_id[:20])
            )
            
            # Step 4: Deploy to Cloud Run with optimal configuration
            region = build_result.get('region', 'us-central1')
            await tracker.start_cloud_deployment(service_name, region)
            
            await tracker.emit_deployment_config(
                optimal_config.cpu,
                optimal_config.memory,
                optimal_config.concurrency
            )
            
            deploy_start = time.time()
            
            async def deploy_progress(data):
                """Forward deployment progress to tracker"""
                if data.get('status'):
                    await tracker.emit_deployment_status(data['status'])
            
            # Add resource configuration to deployment
            deploy_env = env_vars or {}
            
            deploy_result = await self.gcloud_service.deploy_to_cloudrun(
                build_result['image_tag'],
                service_name,
                env_vars=deploy_env,
                progress_callback=deploy_progress,
                user_id=deployment_id[:8]  # Use deployment ID prefix
            )
            
            deploy_duration = time.time() - deploy_start
            self.monitoring.record_stage(deployment_id, 'deploy', 'success', deploy_duration)
            
            if not deploy_result.get('success'):
                await tracker.emit_error('cloud_deployment', deploy_result.get('error', 'Deployment failed'))
                self.monitoring.complete_deployment(deployment_id, 'failed')
                return {
                    'type': 'error',
                    'content': f"âŒ **Deployment failed**\n\n{deploy_result.get('error')}\n\nCheck:\nâ€¢ Cloud Run API is enabled\nâ€¢ Service account permissions",
                    'timestamp': datetime.now().isoformat()
                }
            
            # Success! Complete deployment
            await tracker.complete_cloud_deployment(deploy_result['url'])
            
            # Calculate metrics and complete monitoring
            total_duration = time.time() - start_time
            self.monitoring.complete_deployment(deployment_id, 'success')
            
            # Store deployment info
            self.project_context['deployed_service'] = service_name
            self.project_context['deployment_url'] = deploy_result['url']
            self.project_context['deployment_id'] = deployment_id
            
            # Get cost estimation
            estimated_cost = self.optimization.estimate_cost(optimal_config, 100000)  # 100k requests/month
            
            content = f"""
ðŸŽ‰ **Deployment Successful!**

Your service is now live at:
**{deploy_result['url']}**

**Service:** {service_name}
**Region:** {deploy_result['region']}
**Deployment ID:** `{deployment_id}`

âš¡ **Performance:**
â€¢ Build: {build_duration:.1f}s
â€¢ Deploy: {deploy_duration:.1f}s
â€¢ Total: {total_duration:.1f}s

ðŸ”§ **Configuration:**
â€¢ CPU: {optimal_config.cpu} vCPU
â€¢ Memory: {optimal_config.memory}
â€¢ Concurrency: {optimal_config.concurrency} requests
â€¢ Auto-scaling: {optimal_config.min_instances}-{optimal_config.max_instances} instances

ðŸ’° **Estimated Cost (100k requests/month):**
â€¢ ${estimated_cost['total_monthly']:.2f} USD/month

âœ… Auto HTTPS enabled
âœ… Auto-scaling configured
âœ… Health checks active
âœ… Monitoring enabled

What would you like to do next?
            """.strip()
            
            return {
                'type': 'deployment_complete',
                'content': content,
                'data': {
                    **deploy_result,
                    'metrics': {
                        'build_duration': build_duration,
                        'deploy_duration': deploy_duration,
                        'total_duration': total_duration
                    },
                    'configuration': {
                        'cpu': optimal_config.cpu,
                        'memory': optimal_config.memory,
                        'concurrency': optimal_config.concurrency,
                        'min_instances': optimal_config.min_instances,
                        'max_instances': optimal_config.max_instances
                    },
                    'cost_estimate': estimated_cost,
                    'security_scan': security_scan
                },
                'deployment_url': deploy_result['url'],
                'actions': [
                    {
                        'id': 'view_logs',
                        'label': 'ðŸ“Š View Logs',
                        'type': 'button',
                        'action': 'view_logs'
                    },
                    {
                        'id': 'setup_cicd',
                        'label': 'ðŸ”„ Setup CI/CD',
                        'type': 'button',
                        'action': 'setup_cicd'
                    },
                    {
                        'id': 'custom_domain',
                        'label': 'ðŸŒ Add Custom Domain',
                        'type': 'button',
                        'action': 'custom_domain'
                    }
                ],
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.monitoring.complete_deployment(deployment_id, 'failed')
            print(f"[Orchestrator] Deployment error: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                'type': 'error',
                'content': f'âŒ **Deployment failed**\n\n```\n{str(e)}\n```',
                'timestamp': datetime.now().isoformat()
            }
    
    async def _handle_list_repos(self, progress_callback=None) -> Dict:
        """List user's GitHub repositories - REAL IMPLEMENTATION"""
        
        try:
            # Validate GitHub token first
            token_check = self.github_service.validate_token()
            if not token_check.get('valid'):
                return {
                    'type': 'error',
                    'content': f"âŒ **GitHub token invalid**\n\n{token_check.get('error')}\n\nPlease set `GITHUB_TOKEN` environment variable.\n\nGet token at: https://github.com/settings/tokens",
                    'timestamp': datetime.now().isoformat()
                }
            
            if progress_callback:
                await progress_callback({
                    'type': 'typing',
                    'message': 'Fetching your GitHub repositories...'
                })
            
            repos = self.github_service.list_repositories()
            
            if not repos:
                return {
                    'type': 'message',
                    'content': 'ðŸ“š **No repositories found**\n\nCreate a repository on GitHub first, then try again.',
                    'timestamp': datetime.now().isoformat()
                }
            
            # Format repo list beautifully
            repo_list = '\n'.join([
                f"**{i+1}. {repo['name']}** ({repo['language'] or 'Unknown'})"
                f"\n   {repo.get('description', 'No description')[:60]}"
                f"\n   â­ {repo['stars']} stars | ðŸ”’ {'Private' if repo['private'] else 'Public'}"
                for i, repo in enumerate(repos[:10])
            ])
            
            content = f"""
ðŸ“š **Your GitHub Repositories** ({len(repos)} total)

{repo_list}

Which repository would you like to deploy? Just tell me the name or paste the URL!
            """.strip()
            
            return {
                'type': 'message',
                'content': content,
                'data': {'repositories': repos},
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"[Orchestrator] List repos error: {str(e)}")
            return {
                'type': 'error',
                'content': f'âŒ **Failed to list repositories**\n\n{str(e)}',
                'timestamp': datetime.now().isoformat()
            }
    
    async def _handle_get_logs(self, service_name: str, limit: int = 50, progress_callback=None) -> Dict:
        """Get deployment logs - REAL IMPLEMENTATION"""
        
        if not self.gcloud_service:
            return {
                'type': 'error',
                'content': 'âŒ **Google Cloud not configured**\n\nPlease set `GOOGLE_CLOUD_PROJECT` environment variable.',
                'timestamp': datetime.now().isoformat()
            }
        
        try:
            if progress_callback:
                await progress_callback({
                    'type': 'typing',
                    'message': f'Fetching logs for {service_name}...'
                })
            
            logs = self.gcloud_service.get_service_logs(service_name, limit=limit)
            
            if not logs or len(logs) == 0:
                return {
                    'type': 'message',
                    'content': f'ðŸ“Š **No logs found for {service_name}**\n\nService may not have received traffic yet.',
                    'timestamp': datetime.now().isoformat()
                }
            
            # Format logs
            log_output = '\n'.join(logs[-20:])  # Last 20 entries
            
            content = f"""
ðŸ“Š **Logs for {service_name}**

```
{log_output}
```

Showing last {min(20, len(logs))} entries (total: {len(logs)})
            """.strip()
            
            return {
                'type': 'message',
                'content': content,
                'data': {'logs': logs},
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"[Orchestrator] Get logs error: {str(e)}")
            return {
                'type': 'error',
                'content': f'âŒ **Failed to fetch logs**\n\n{str(e)}',
                'timestamp': datetime.now().isoformat()
            }
    
    # ========================================================================
    # CONTEXT MANAGEMENT
    # ========================================================================
    
    def _build_context_prefix(self) -> str:
        """Build context string from stored project data"""
        if not self.project_context:
            return ""
        
        context_parts = []
        if 'framework' in self.project_context:
            context_parts.append(f"Framework: {self.project_context['framework']}")
        if 'language' in self.project_context:
            context_parts.append(f"Language: {self.project_context['language']}")
        if 'deployed_service' in self.project_context:
            context_parts.append(f"Deployed Service: {self.project_context['deployed_service']}")
        if 'project_path' in self.project_context:
            context_parts.append(f"Project Path: {self.project_context['project_path']}")
        
        return "Current project context: " + ", ".join(context_parts) if context_parts else ""
    
    def update_context(self, key: str, value: any):
        """Update project context"""
        self.project_context[key] = value
    
    def get_context(self) -> Dict:
        """Get current project context"""
        return self.project_context


# Test orchestrator with real services
async def test_orchestrator():
    import os
    from dotenv import load_dotenv
    load_dotenv()
    
    orchestrator = OrchestratorAgent(
        gemini_api_key=os.getenv('GEMINI_API_KEY'),
        github_token=os.getenv('GITHUB_TOKEN'),
        gcloud_project=os.getenv('GOOGLE_CLOUD_PROJECT')
    )
    
    test_messages = [
        "List my GitHub repositories",
        "Analyze my repo: https://github.com/user/flask-app",
        "Deploy it to Cloud Run as my-flask-service"
    ]
    
    for msg in test_messages:
        print(f"\n{'='*60}")
        print(f"ðŸ§‘ USER: {msg}")
        print(f"{'='*60}")
        response = await orchestrator.process_message(msg, session_id="test-123")
        print(f"ðŸ¤– SERVERGEM:\n{response['content']}")

if __name__ == "__main__":
    asyncio.run(test_orchestrator())